{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import html\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "\n",
    "setup_seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    if isinstance(raw_text, list):\n",
    "        new_raw_text = []\n",
    "        for raw in raw_text:\n",
    "            raw = html.unescape(raw)\n",
    "            raw = re.sub(r'</?\\w+[^>]*>', '', raw)\n",
    "            raw = re.sub(r'[\"\\n\\r]*', '', raw)\n",
    "            new_raw_text.append(raw.strip())\n",
    "        cleaned_text = ' '.join(new_raw_text)\n",
    "    else:\n",
    "        if isinstance(raw_text, dict):\n",
    "            cleaned_text = str(raw_text)[1:-1].strip()\n",
    "        else:\n",
    "            cleaned_text = raw_text.strip()\n",
    "        cleaned_text = html.unescape(cleaned_text)\n",
    "        cleaned_text = re.sub(r'</?\\w+[^>]*>', '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'[\"\\n\\r]*', '', cleaned_text)\n",
    "    index = -1\n",
    "    while -index < len(cleaned_text) and cleaned_text[index] == '.':\n",
    "        index -= 1\n",
    "    index += 1\n",
    "    if index == 0:\n",
    "        cleaned_text = cleaned_text + '.'\n",
    "    else:\n",
    "        cleaned_text = cleaned_text[:index] + '.'\n",
    "    if len(cleaned_text) >= 2000:\n",
    "        cleaned_text = ''\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feats = []\n",
    "with gzip.open('../orig_data/meta_Electronics.json.gz', \"r\") as fp:\n",
    "    for idx, line in tqdm(enumerate(fp), desc=\"Load metas\"):\n",
    "        \n",
    "        # data = json.loads(line)\n",
    "        data = eval(line) # 2014\n",
    "        item = data[\"asin\"]\n",
    "\n",
    "        if 'title' in data.keys():\n",
    "            title = clean_text(data[\"title\"])\n",
    "        else:\n",
    "            title = ''\n",
    "\n",
    "        if 'description' in data.keys():\n",
    "            descriptions = data[\"description\"]\n",
    "            descriptions = clean_text(descriptions)\n",
    "        else:\n",
    "            descriptions = ''\n",
    "\n",
    "        if 'brand' in data.keys():\n",
    "            brand = data[\"brand\"].replace(\"by\\n\", \"\").strip()\n",
    "        else:\n",
    "            brand = ''\n",
    "\n",
    "        if 'category' in data.keys():\n",
    "            category_key = 'category'\n",
    "        elif 'categories' in data.keys():\n",
    "            category_key = 'categories'\n",
    "        else:\n",
    "            category_key = None\n",
    "\n",
    "        if category_key:\n",
    "            categories = data[category_key]\n",
    "            if category_key == 'categories':\n",
    "                categories = sum(categories,[])\n",
    "            \n",
    "            new_categories = []\n",
    "            for category in categories:\n",
    "                if \"</span>\" in category:\n",
    "                    break\n",
    "                new_categories.append(category.strip())\n",
    "            categories = \",\".join(new_categories).strip()\n",
    "        else:\n",
    "            categories = ''\n",
    "\n",
    "        item_feats.append({\n",
    "            \"asin\": item,\n",
    "            \"title\": title,\n",
    "            \"description\": descriptions,\n",
    "            \"brand\": brand,\n",
    "            \"categories\": categories\n",
    "        })\n",
    "item_df = pd.DataFrame(item_feats)\n",
    "item_df.head()\n",
    "item_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df.drop_duplicates(subset=['asin'])\n",
    "item_df = item_df.reset_index(drop=True)\n",
    "item_df.head()\n",
    "item_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_text(row):\n",
    "    return ' '.join([row['title'], row['description']])\n",
    "    \n",
    "item_df['text'] = item_df.apply(get_item_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_null_text(row):\n",
    "    row_text = row['text'].strip()\n",
    "    return len(row_text)==0\n",
    "\n",
    "item_df_null_text = item_df.apply(count_null_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df[~item_df_null_text].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df[~item_df_null_text].reset_index(drop=True)\n",
    "item_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rec Inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID, itemID, score, ts = [],[],[],[]\n",
    "with gzip.open('../orig_data/reviews_Electronics_5.json.gz') as f:\n",
    "    for l in tqdm(f, desc=\"Load rec inter\"):\n",
    "        line = json.loads(l.strip())\n",
    "        userID.append(line['reviewerID'])\n",
    "        itemID.append(line['asin'])\n",
    "        score.append(line['overall'])\n",
    "        ts.append(line['unixReviewTime'])\n",
    "    \n",
    "rec_inter = pd.DataFrame(\n",
    "    data = list(zip(userID, itemID, score, ts)), columns=['user_id','item_id','score','ts']\n",
    ").sort_values(by=['user_id','ts']).reset_index(drop=True)\n",
    "rec_inter.head()\n",
    "rec_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rec_inter = rec_inter[rec_inter['item_id'].isin(item_df['asin'].tolist())].reset_index(drop=True)\n",
    "filter_rec_inter.head()\n",
    "filter_rec_inter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Src Inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_id_list, q_id_list, i_id_list, label_list = [],[],[],[]\n",
    "with gzip.open('../orig_data/train.qrels.gz') as f:\n",
    "    for l in tqdm(f):\n",
    "        u_q_id, _, i_id, label = l.strip().decode().split(' ')\n",
    "        u_id, q_id = u_q_id.split(\"_\")\n",
    "        u_id_list.append(u_id)\n",
    "        q_id_list.append(int(q_id))\n",
    "        i_id_list.append(i_id)\n",
    "        label_list.append(label)\n",
    "\n",
    "train_src = pd.DataFrame(\n",
    "    data = list(zip(u_id_list, q_id_list, i_id_list, label_list)), columns=['user_id','query_id','item_id','label']\n",
    ")\n",
    "train_src.head()\n",
    "train_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_id_list, q_id_list, i_id_list, label_list = [],[],[],[]\n",
    "with gzip.open('../orig_data/test.qrels.gz') as f:\n",
    "    for l in tqdm(f):\n",
    "        u_q_id, _, i_id, label = l.strip().decode().split(' ')\n",
    "        u_id, q_id = u_q_id.split(\"_\")\n",
    "        u_id_list.append(u_id)\n",
    "        q_id_list.append(int(q_id))\n",
    "        i_id_list.append(i_id)\n",
    "        label_list.append(label)\n",
    "\n",
    "test_src = pd.DataFrame(\n",
    "    data = list(zip(u_id_list, q_id_list, i_id_list, label_list)), columns=['user_id','query_id','item_id','label']\n",
    ")\n",
    "test_src.head()\n",
    "test_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_src_inter = pd.concat([train_src,test_src],axis=0).reset_index(drop=True)\n",
    "all_src_inter.shape\n",
    "all_src_inter.tail()\n",
    "\n",
    "df = shuffle(all_src_inter)\n",
    "\n",
    "new_src_Data = df.drop_duplicates(subset=['user_id', 'item_id'], keep='first')\n",
    "new_src_Data.head()\n",
    "new_src_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_src_inter = rec_inter.merge(new_src_Data,how='inner',on=['user_id','item_id'])\n",
    "merge_src_inter.shape\n",
    "merge_src_inter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_merge_src_inter = merge_src_inter[merge_src_inter['item_id'].isin(item_df['asin'].tolist())].reset_index(drop=True)\n",
    "filter_merge_src_inter.shape\n",
    "filter_merge_src_inter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Item Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_item_set = set(rec_inter['item_id'].unique())\n",
    "src_item_set = set(merge_src_inter['item_id'].unique())\n",
    "len(rec_item_set)\n",
    "len(src_item_set)\n",
    "len(rec_item_set & src_item_set)\n",
    "len(rec_item_set | src_item_set)\n",
    "\n",
    "all_item_set = list(rec_item_set | src_item_set)\n",
    "len(all_item_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df[item_df['asin'].isin(all_item_set)].reset_index(drop=True)\n",
    "item_df.head()\n",
    "item_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df.astype({\"asin\":'category'})\n",
    "id2item = item_df['asin'].cat.categories.to_list()\n",
    "# id2item[0]\n",
    "item2id = {id2item[k]:k+1 for k in range(len(id2item))} # +1 for padding\n",
    "# item2id[0]\n",
    "item_df['item_id'] = item_df['asin'].map(item2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_item_df = pd.DataFrame({\"item_id\":[0], \"asin\":[\"<pad>\"], \"title\":[\"\"], \"description\":[\"\"], \"brand\":[\"\"], \"categories\":[\"\"]})\n",
    "pad_item_df\n",
    "\n",
    "all_item_df = pd.concat([pad_item_df, item_df],axis=0)\n",
    "all_item_df['item_id'] = all_item_df['item_id'].astype('int')\n",
    "all_item_df = all_item_df.sort_values(by=['item_id'])\n",
    "all_item_df = all_item_df.reset_index(drop=True)\n",
    "\n",
    "all_item_df['item_id'].nunique()\n",
    "all_item_df.head()\n",
    "all_item_df.shape\n",
    "\n",
    "all_item_df.to_pickle('../raw_data/item_info.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get User Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_user_set = set(filter_rec_inter['user_id'].unique())\n",
    "src_user_set = set(filter_merge_src_inter['user_id'].unique())\n",
    "len(rec_user_set)\n",
    "len(src_user_set) \n",
    "len(rec_user_set & src_user_set)\n",
    "len(rec_user_set | src_user_set)\n",
    "\n",
    "all_user_set = list(rec_user_set & src_user_set)\n",
    "len(all_user_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info = pd.DataFrame(data=all_user_set, columns=['user'])\n",
    "# user_info.head()\n",
    "\n",
    "user_info = user_info.astype({\"user\":'category'})\n",
    "id2user = user_info['user'].cat.categories.to_list()\n",
    "# id2user[0]\n",
    "user2id = {id2user[k]:k for k in range(len(id2user))}\n",
    "\n",
    "user_info['user_id'] = user_info['user'].map(user2id)\n",
    "user_info.head()\n",
    "user_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rec_inter = filter_rec_inter[filter_rec_inter['user_id'].isin(all_user_set)].reset_index(drop=True)\n",
    "filter_rec_inter.shape\n",
    "\n",
    "filter_merge_src_inter = filter_merge_src_inter[filter_merge_src_inter['user_id'].isin(all_user_set)].reset_index(drop=True)\n",
    "filter_merge_src_inter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map user/item to ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rec_inter['user_id'] = filter_rec_inter['user_id'].map(user2id)\n",
    "filter_rec_inter['item_id'] = filter_rec_inter['item_id'].map(item2id)\n",
    "filter_rec_inter.head()\n",
    "filter_rec_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_merge_src_inter['user_id'] = filter_merge_src_inter['user_id'].map(user2id)\n",
    "filter_merge_src_inter['item_id'] = filter_merge_src_inter['item_id'].map(item2id)\n",
    "filter_merge_src_inter.head()\n",
    "filter_merge_src_inter.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Src Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = []\n",
    "with gzip.open('../orig_data/query_text.txt.gz') as f:\n",
    "    for l in tqdm(f):\n",
    "        query_text.append(l.strip().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {'<pad>': 0}\n",
    "id2word = ['<pad>']\n",
    "\n",
    "query2id = {'<pad>': 0}\n",
    "id2query = [{\"query\":\"<pad>\", \"words\":[\"<pad>\"], \"words_id\":[0]}]\n",
    "\n",
    "session_idx = 0\n",
    "\n",
    "search_session_info_list = []\n",
    "src_inter_list = []\n",
    "for _, line in tqdm(filter_merge_src_inter.iterrows()):\n",
    "    session_idx += 1\n",
    "\n",
    "    user_id = line['user_id']\n",
    "    session_id = session_idx\n",
    "    click_item_ls = [line['item_id']]\n",
    "    this_ts = line['ts']\n",
    "    \n",
    "    query = query_text[line['query_id']]\n",
    "    query_words = query.split(' ')\n",
    "\n",
    "    for word in query_words:\n",
    "        if word not in word2id.keys():\n",
    "            word2id[word] = len(word2id)\n",
    "            id2word.append(word)\n",
    "    \n",
    "    query_words_id = [word2id[x] for x in query_words]\n",
    "\n",
    "    if query not in query2id.keys():\n",
    "        query2id[query] = len(query2id)\n",
    "        id2query.append({\"query\":query, \n",
    "                         \"words\":query_words, \n",
    "                         \"words_id\":query_words_id})\n",
    "        \n",
    "\n",
    "    search_session_info_list.append({\n",
    "        \"search_session_id\": session_id,\n",
    "        \"pos_items\": click_item_ls,\n",
    "        \"keyword\": query_words_id,\n",
    "        'query_id':query2id[query],\n",
    "        'click_list':[1],\n",
    "        'time_list':[this_ts]\n",
    "    })\n",
    "\n",
    "    src_inter_list.append({\n",
    "        \"user_id\": user_id,\n",
    "        \"item_id\": click_item_ls[0],\n",
    "        \"search_session_id\": session_id,\n",
    "        \"ts\": this_ts,\n",
    "        \"keyword\": query_words_id,\n",
    "        'query_id':query2id[query]\n",
    "    })\n",
    "\n",
    "    \n",
    "search_session_info = pd.DataFrame(search_session_info_list)\n",
    "search_session_info.head()\n",
    "search_session_info.shape\n",
    "\n",
    "src_inter = pd.DataFrame(src_inter_list)\n",
    "src_inter.head()\n",
    "src_inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2query[0]\n",
    "id2query[1]\n",
    "\n",
    "id2word[0]\n",
    "id2word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_rec_inter.to_pickle(\"../raw_data/rec_inter.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_inter.to_pickle('../raw_data/src_inter.pkl')\n",
    "search_session_info.to_pickle('../raw_data/session_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info.to_pickle('../raw_data/user_profile.pkl')\n",
    "all_item_df.to_pickle('../raw_data/item_info.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(id2query, open(\"../vocab/query_vocab.pkl\",'wb'))\n",
    "pickle.dump(id2word, open(\"../vocab/word_vocab.pkl\",'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JOINT_SAR",
   "language": "python",
   "name": "joint_sar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
